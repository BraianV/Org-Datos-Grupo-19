{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensamble de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from scripts import mejor_featurer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, KFold,RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, recall_score, accuracy_score,precision_score ,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/mlard/OneDrive/Escritorio/Mateo/Org de Datos/tp3Github/Org-Datos-Grupo-19/dataset-procesado.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trabajo = mejor_featurer.featurizer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separo en train y test para trabajar los modelos\n",
    "df_trabajo_x=df_trabajo.drop(['target'], axis='columns', inplace=False)\n",
    "\n",
    "df_trabajo_y = df_trabajo['target'].copy()\n",
    "#Uso parametro stratify para balancear el target dentro de cada ser de datos de train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_trabajo_x, df_trabajo_y, test_size=0.3, random_state=1, stratify = df_trabajo_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos el modelo Random Forest y el XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv = RandomForestClassifier(random_state=1, n_jobs=-1)\n",
    "\n",
    "param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n",
    "               \"min_samples_leaf\" : [1, 5], \n",
    "               \"max_features\":[\"sqrt\", \"log2\"],\n",
    "               \"min_samples_split\" : [2, 5], \n",
    "               \"n_estimators\": [10,50] }\n",
    "\n",
    "#Probamos entrenando sólo con 1 métrica\n",
    "gs = GridSearchCV(estimator=rf_cv, param_grid=param_grid, scoring=\"roc_auc\", cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_fit = gs.fit(X = x_train, y = y_train)\n",
    "best_params_rfc = gs_fit.best_params_\n",
    "\n",
    "print(best_params_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_best_model = RandomForestClassifier(**best_params_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos lo mismo pero con el XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from scripts import featurizer_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trabajo_xgb = featurizer_xgb.featurizer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xgb = df_trabajo_xgb.drop(axis = 1, columns = ['target'])\n",
    "y_xgb = df_trabajo_xgb.target\n",
    "\n",
    "features = X_xgb.columns\n",
    "\n",
    "#Set hyperparameters dictionary\n",
    "param_dist = {'n_estimators':[50, 100],\n",
    "              'max_depth':[5,10],              \n",
    "              'subsample': [0.5,0.8,1],\n",
    "              'colsample_bytree': [0.5,0.8,1],              \n",
    "              'learning_rate':[0.01, 0.1, 0.3]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "random_search_xgb = RandomizedSearchCV(xgb,\n",
    "                                       param_distributions = param_dist,\n",
    "                                       n_iter = 2,\n",
    "                                       scoring = 'roc_auc',\n",
    "                                       cv = 5,\n",
    "                                       n_jobs = -1,\n",
    "                                       verbose=5)\n",
    "random_search_xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt_xgb = random_search_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbModelBestParams = XGBClassifier(**params_opt_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vot_clf = VotingClassifier(estimators = [('rnd', rfc_best_model),('xgb', xgbModelBestParams)], voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vot_clf.fit(x_train, y_train)\n",
    "pred_vot_clf = vot_clf.predict(x_test)\n",
    "\n",
    "df_resultados_vot_clf=pd.DataFrame(zip(y_test,pred_vot_clf),columns=['test','predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla=confusion_matrix(df_resultados_vot_clf['test'], df_resultados_vot_clf['predicted'])\n",
    "\n",
    "print('Tabla de confusion de test: ')\n",
    "\n",
    "grf=sns.heatmap(tabla,cmap='Blues',annot=True,fmt='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_vot_clf = vot_clf.predict(x_train)\n",
    "tabla=confusion_matrix(y_train, y_train_vot_clf)\n",
    "print('Tabla de confusion de entrenamiento: ')\n",
    "grf=sns.heatmap(tabla,cmap='Blues',annot=True,fmt='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_comparation(df_predict):\n",
    "    df_predict[\"success\"] = (df_predict[\"test\"] == df_predict[\"predicted\"])\n",
    "    df_predict[\"success\"].value_counts().plot.pie(autopct='%1.1f%%')\n",
    "    plt.title(\"Cantidad de valores predecidos correctamente\") \n",
    "    plt.show()\n",
    "\n",
    "    precicion = precision_score(df_predict['test'], df_predict['predicted'])\n",
    "    f1=f1_score(df_predict['test'], df_predict['predicted'])\n",
    "    recall = recall_score(df_predict['test'], df_predict['predicted'])\n",
    "    \n",
    "    print(f\"Precision score: \\t{round(precicion,3)}\")\n",
    "    print(f\"Recall score: \\t\\t{round(recall,3)}\")\n",
    "    print(f\"F1 score: \\t\\t{round(f1,3)}\")\n",
    "\n",
    "    accuracy=accuracy_score(df_predict['test'], df_predict['predicted'], normalize=True)\n",
    "    print(\"Tiene una accuracy del: \" + str(round(accuracy, 3)) + \"% como muestra el pie chart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy de entrenamiento : \", accuracy_score(y_train, y_train_vot_clf))\n",
    "print(\"Accuracy de test : \", accuracy_score(df_resultados_vot_clf['test'], df_resultados_vot_clf['predicted']))\n",
    "print('\\n')\n",
    "print('Se puede ver que la precicion para el voting classifier no tiene overfitting ni underfitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_comparation(df_resultados_vot_clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f7ef91bea4ea4c0666122001abe4a5fedf50b5f15e9241244512c5f056ca9c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
